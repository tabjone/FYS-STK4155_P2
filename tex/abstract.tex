
\begin{abstract}
    %% TODO: IDK how good this sentence fits here now?
    In this project we did implement our own Feed Forward Neural Network (FFNN)
    with numerous optimization methods.    
    

    %%%% FEEL FREE TO REMOVE THIS AND RE-WRITE
    %GD METHODS
    We started by fitting a second order polynomial with the gradient decent
    methods of plain GD and stochastic with the AdaGrad and ADAM and RMSProp
    optimizers. We found that the stochastic methods in general converged
    faster (less iterations to reach low MSE), the more number of minibatches
    we used.

    And when looking at momentum
    we found that in general we could have higher learning rates when the
    momentum parameter was closer to one. But for the plain GD it was
    especially important with a low learning rate.

    %%NN
    When moving over to polynomial fitting with a Neural Network (NN) we used a
    dense neural network with one hidden layer of 5 neurons and looked at
    different activation functions. We found the Sigmoid function gave a by eye
    almost perfect fit after 2000 epochs and the MSE showed that all of the
    activations produced a good fit. And decreasing the learning rate produced
    better MSE. 


    % Classification 
    In the last part we did a classification analysis on the Wisconsin Breast
    Cancer data. We experimented with different hyperparameter combinations.
    In our last run we tried to find the overall best parameter combination. 
    This resulted in an accuracy score of 0.9825 on our test data. According to
    the literature the result is better than what is to be expected for the
    Wisconsin Breast Cancer data. The reason is that we did not include any re
    sampling methods when we trained our model.

    We did find an error in the calculation of our L2 regularization parameter,
    $\lambda$. $\lambda $ was multiplied with the gradient of the weights
    instead of the weights from the previous iteration, effectively acting as
    an increased learning rate. Our analysis of the L2-regularization parameter
    is therefore invalid and should not be cited.  

\end{abstract}

\begin{comment}
    The abstract gives the reader a quick overview of what has been done and the most important results.

\end{comment}

