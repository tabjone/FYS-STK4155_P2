%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

\begin{comment}
State your main findings and interpretations. 
Try as far as possible to present perspectives for future work. 
Try to discuss the pros and cons of the methods and possible improvements.
\\~\\
Main findings:
    gradient methods: 
        Fitted second order polynomial.
        Found that smaller learning gave better MSE and L2-regularization gave
        worse MSE. Because optimizer used same order L2 gave worse results.
        Because lambda counters overfitting at the cost of poorer training fit.
        And here overfit was no problem. Future work is to test on more
        datasets.

        Also found that for momentum parameter everyone got better MSE scores
        except adam.

        
        all methods had about the same convergence, except adam which was slow
        on the polynomial dataset with no noise.
        Possible future work is to run this on more complex datasets.
        
        Bigger minibatch sizes made faster convergence.


    Neural network (regression):
        Sigmoid: Lambda did not play huge role for big learning rates, but for
        small for smaller the MSE got decreased.
        Relu: Not much impact by lambda. Same with leaky.
        
        Now sklearn: Lambda increased, MSE increase for sigmoid. Not much
        effect for relu.

        Overall we see that learning rate plays huge role. But not lambda, for
        regression.
        

    Classifiction:
        Minibatches: Clear trend of faster convergence when increasing number
        of mini-batches


Perspectives for future work:
    ...

Pros/Cons of methods and possible improvments:
Was the models used good? What worked and what didn't?
    gradient methods: 
        adam optimizer, not good for not noisy data, slow convergence

    neural network regression:
        We used a dense neural network. This could be made sparse
        

    classification:
        use of sigmoid activation for output in classification. Could be replaced
        by softmax for classifying non-binary problems.
    


\end{comment}

% Possible directions and future improvements?
% NN classification
% * should have controlled random events better, such as: 
%   * initialziation of weights, biases 
%   * selection of minibatces
% Not sure how this affected the results and if the small variations in some
% parameters can be attributed to random events   

