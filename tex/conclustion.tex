%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

%GD methods
We fit a second order polynomial using a variety of GD methods. Specifically we
looked at regular GD, momentum GD, plain SGD, momentum SGD and also using AdaGrad, Adam 
and RMSProp for tuning the learning rate. We found that for a momentum
of 0.8 decreasing the learning rate always led to a decrease in the MSE, but
for momentum values of around 0.4 a higher learning rate of about 0.6-0.8 would
produce better results. In general the lower the momentum was, the higher we
could have our learning rates. But this could be due to the simplicity of the
dataset and should be explored further in future work. The Adam method did not
benefit from adding a momentum, which may be explained by it already having a
first and second moment. 

We also found that adding a L2-regularization gave worse MSE. 

For increasing batch sizes of the stochastic methods we found that all methods
converged to low MSE values faster. ADAM preformed quite a lot worse than the other 
methods. This could be because Adam is better at noisy datasets, and we tried to
fit a second order polynomial without any noise. This should be explored in future
work, using different datasets.


%Neural network (regression)
Then we tried to fit a neural network to the same polynomial. We used a dense
neural network with one hidden layer and 5 neurons, with sigmoid, ReLU and
leaky ReLU as activation functions for the hidden layer. Each activation
function had its advantages and disadvantages with Sigmoid obtaining the best MSE score, but being
most probable to get stuck on poor MSE scores, while the opposite was true for leaky ReLU, and 
was in the midle with regards to both properties. 
The L2-parameter was discovered to be implemented in a wrong way, such that it effectively changed the 
learning rate. For future work we would try implementing a sparse neural network to
decrease the number of computations required for a forward and backward pass of
the network. 

The resulting polynomial fit after 2000 iterations from our neural network were pretty similar to the target, allthough deviations
were clearly visible. The neural network did a lot worse than gradient descent, but was able to give a good fit 
without using a design matrix which were used by gradient descent. 


\begin{comment}
State your main findings and interpretations. 
Try as far as possible to present perspectives for future work. 
Try to discuss the pros and cons of the methods and possible improvements.
\\~\\
Main findings:

    Neural network (regression):
        Sigmoid: Lambda did not play huge role for big learning rates, but for
        small for smaller the MSE got decreased.
        Relu: Not much impact by lambda. Same with leaky.
        
        Now sklearn: Lambda increased, MSE increase for sigmoid. Not much
        effect for relu.

        Overall we see that learning rate plays huge role. But not lambda, for
        regression.
        
Perspectives for future work:
    Sparse network. 

Pros/Cons of methods and possible improvments:
Was the models used good? What worked and what didn't?
    gradient methods: 
        adam optimizer, not good for not noisy data, slow convergence

    neural network regression:
        We used a dense neural network. This could be made sparse
        
    classification:
        use of sigmoid activation for output in classification. Could be replaced
        by softmax for classifying non-binary problems.

Con/improvement: wrong expression for L2-parameter.
    


\end{comment}

% Possible directions and future improvements? XXX: See Discussion
% NN classification
% * should have controlled random events better, such as: 
%   * initialziation of weights, biases 
%   * selection of minibatces
% Not sure how this affected the results and if the small variations in some
% parameters can be attributed to random events   

% Classification
In our classification problem with our FFNN on the Wisconsin Breast Cancer
data, we did experiment with different hyperparameters. We found that the most
influential parameters was mini batch size, momentum, and tuning method.  
10-20 mini batches was superior compared with fewer mini batches. Added
momentum significantly improved our SGD (constant learning rate) and
Adagrad method. Other tuning method such RMS prop and Adam did not benefit from
added momentum. All tree tuning methods did preform similar and should be tested on a NN classification
problem. Adagrad benefits from a higher learning rate than RMS prop and Adam.    

After experimentation width different parameters, we manged to obtain an
accuracy score of 0.9825 on the test data. According to the literature this is
better than to be expected. One significant weakness with our approach is that
we did not utilize any re-sampling technique. Thus, our high accuracy is probably a
result of a lucky selection of training- and test-data. 

Random events such a mini batch selection and weights selection should have
been better controlled. Then, we could have more accurately determined if the
variations in our scores was due to hyperparameter selection or random events.
Re-sampling techniques could have been utilized to obtain this insight. 

The same classification analysis obtained with logistic regression preformed
worse, with an accuracy score of 0.9035. However, we manged to obtain an accuracy
of 0.9649 with sci-kit learn (python package). Why the two results differ is
not clear. Overall classification with a FFNN outperforms logistic regression. 






