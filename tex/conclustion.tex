%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

\begin{comment}
State your main findings and interpretations. 
Try as far as possible to present perspectives for future work. 
Try to discuss the pros and cons of the methods and possible improvements.
\\~\\
Main findings:
    gradient methods: 
        Fitted second order polynomial.
        Found that smaller learning gave better MSE and L2-regularization gave
        worse MSE. Because optimizer used same order L2 gave worse results.
        Because lambda counters overfitting at the cost of poorer training fit.
        And here overfit was no problem. Future work is to test on more
        datasets.

        Also found that for momentum parameter everyone got better MSE scores
        except adam.

        all methods had about the same convergence, except adam which was slow
        on the polynomial dataset with no noise.
        Possible future work is to run this on more complex datasets.
        
        Bigger minibatch sizes made faster convergence.

    Neural network (regression):
        Sigmoid: Lambda did not play huge role for big learning rates, but for
        small for smaller the MSE got decreased.
        Relu: Not much impact by lambda. Same with leaky.
        
        Now sklearn: Lambda increased, MSE increase for sigmoid. Not much
        effect for relu.

        Overall we see that learning rate plays huge role. But not lambda, for
        regression.
        
Perspectives for future work:
    Sparse network. 

Pros/Cons of methods and possible improvments:
Was the models used good? What worked and what didn't?
    gradient methods: 
        adam optimizer, not good for not noisy data, slow convergence

    neural network regression:
        We used a dense neural network. This could be made sparse
        
    classification:
        use of sigmoid activation for output in classification. Could be replaced
        by softmax for classifying non-binary problems.

Con/improvement: wrong expression for L2-parameter.
    


\end{comment}

% Possible directions and future improvements? XXX: See Discussion
% NN classification
% * should have controlled random events better, such as: 
%   * initialziation of weights, biases 
%   * selection of minibatces
% Not sure how this affected the results and if the small variations in some
% parameters can be attributed to random events   

% Classification
In our classification problem with our FFNN on the Wisconsin Breast Cancer
data, we did experiment with different hyperparameters. We found that the most
influential parameters was mini batch size, momentum, and tuning method.  
10-20 mini batches was superior compared with fewer mini batches. Added
momentum significantly improved our SGD (constant learning rate) and
Adagrad method. Other tuning method such RMS prop and Adam did not benefit from
added momentum. All tree tuning methods did preform similar and should be tested on a NN classification
problem. Adagrad benefits from a higher learning rate than RMS prop and Adam.    

After experimentation width different parameters, we manged to obtain an
accuracy score of 0.9825 on the test data. According to the literature this is
better than to be expected. One significant weakness with our approach is that
we did not utilize any re-sampling technique. Thus, our high accuracy is probably a
result of a lucky selection of training- and test-data. 

Random events such a mini batch selection and weights selection should have
been better controlled. Then, we could have more accurately determined if the
variations in our scores was due to hyperparameter selection or random events.
Re-sampling techniques could have been utilized to obtain this insight. 

The same classification analysis obtained with logistic regression preformed
worse, with an accuracy score of 0.9035. However, we manged to obtain an accuracy
of 0.9649 with sci-kit learn (python package). Why the two results differ is
not clear. Overall classification with a FFNN outperforms logistic regression. 






