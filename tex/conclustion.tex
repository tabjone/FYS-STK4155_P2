%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

%GD methods
We fit a second order polynomial using a variety of GD methods. Specifically we
looked at regular GD, momentum GD, plain, SGD, momentum SGD and also using AdaGrad, Adam 
and RMSProp for tuning the learning rate of SGD. We found that for all methods
a smaller learning rate gave better MSE and that adding a L2-regularization
gave worse MSE. This is likely due to an error in our program and should be
looked at more in future work. We also found that adding a momentum parameter
gave better MSE scores for all methods except for Adam, which it is unclear why
does not benefit from this.
%TODO: Why does adam not benefit from momentum? (Maybe sentence over is stupid)
For increasing batch sizes of the stochastic methods we found that all methods
converged to low MSE values faster, the higher the batch sizes, except for
Adam. This could be because Adam is better at noisy datasets, and we tried to
fit a second order polynomial without any noise using second order parameters,
so that we in theory could get a perfect fit. This should be explored in future
work, using different datasets.


%Neural network (regression)
Then we tried to fit a neural network to the same polynomial. We used a dense
neural network  


\begin{comment}
State your main findings and interpretations. 
Try as far as possible to present perspectives for future work. 
Try to discuss the pros and cons of the methods and possible improvements.
\\~\\
Main findings:

    Neural network (regression):
        Sigmoid: Lambda did not play huge role for big learning rates, but for
        small for smaller the MSE got decreased.
        Relu: Not much impact by lambda. Same with leaky.
        
        Now sklearn: Lambda increased, MSE increase for sigmoid. Not much
        effect for relu.

        Overall we see that learning rate plays huge role. But not lambda, for
        regression.
        
Perspectives for future work:
    Sparse network. 

Pros/Cons of methods and possible improvments:
Was the models used good? What worked and what didn't?
    gradient methods: 
        adam optimizer, not good for not noisy data, slow convergence

    neural network regression:
        We used a dense neural network. This could be made sparse
        
    classification:
        use of sigmoid activation for output in classification. Could be replaced
        by softmax for classifying non-binary problems.

Con/improvement: wrong expression for L2-parameter.
    


\end{comment}

% Possible directions and future improvements? XXX: See Discussion
% NN classification
% * should have controlled random events better, such as: 
%   * initialziation of weights, biases 
%   * selection of minibatces
% Not sure how this affected the results and if the small variations in some
% parameters can be attributed to random events   

% Classification
In our classification problem with our FFNN on the Wisconsin Breast Cancer
data, we did experiment with different hyperparameters. We found that the most
influential parameters was mini batch size, momentum, and tuning method.  
10-20 mini batches was superior compared with fewer mini batches. Added
momentum significantly improved our SGD (constant learning rate) and
Adagrad method. Other tuning method such RMS prop and Adam did not benefit from
added momentum. All tree tuning methods did preform similar and should be tested on a NN classification
problem. Adagrad benefits from a higher learning rate than RMS prop and Adam.    

After experimentation width different parameters, we manged to obtain an
accuracy score of 0.9825 on the test data. According to the literature this is
better than to be expected. One significant weakness with our approach is that
we did not utilize any re-sampling technique. Thus, our high accuracy is probably a
result of a lucky selection of training- and test-data. 

Random events such a mini batch selection and weights selection should have
been better controlled. Then, we could have more accurately determined if the
variations in our scores was due to hyperparameter selection or random events.
Re-sampling techniques could have been utilized to obtain this insight. 

The same classification analysis obtained with logistic regression preformed
worse, with an accuracy score of 0.9035. However, we manged to obtain an accuracy
of 0.9649 with sci-kit learn (python package). Why the two results differ is
not clear. Overall classification with a FFNN outperforms logistic regression. 






