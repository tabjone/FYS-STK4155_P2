\section{Method}
%TODO test gradient methods against analytical solution or regression methods.
%TODO create time vs dataset size for different methods with variance as
%errorbars. So run a couple of times 
\begin{comment}
	Describe the methods and algorithms. You need to
	explain how you implemented the methods and also
	say something about the structure of your algorithm
	and present some parts of your code. You should
	plug in some calculations to demonstrate your code,
	such as selected runs used to validate and verify your
	results. The latter is extremely important! A reader
	needs to understand that your code reproduces selected
	benchmarks and reproduces previous results, either
	numerical and/or well-known closed form expressions.
\end{comment}

\subsection{Cost-function}
%NOT STARTED


\subsection{Regression methods}
%DONE
Say we have a response $\mathbf{y}\in\mathbb{R}^n$ to $p$-number of features $\mathbf{x}_i=[x_{i0}, x_{i1},...,x_{ip-1}]$,
where the set of these $\mathbf{X}=[\mathbf{x}_{0}\ \mathbf{x}_{1}\ ...\ \mathbf{x}_{n-1}]$ is called the design matrix of the model. 
The point of regression analysis is then to find a assume linear relationship between $\mathbf{X}$ and $\mathbf{y}$. 
This assumption gives rise to the linear regression model where $\boldsymbol\beta=\left[\beta_0, \beta_1, ..., \beta_{p-1} \right]^T$ 
are the regression parameters and the error variable $\boldsymbol\epsilon$ is an unobserved random variable that adds 
"noise" to the linear relationship between the dependent variable and regressors. This gives the model
\begin{equation*}
	\tilde{y}(x_i) = \sum_{j=0}^{p-1} \beta_j x_{ij}=\mathbf X_{i*}\boldsymbol{\beta}.
\end{equation*}
Or on vector form
\begin{equation*}
\boldsymbol{\tilde y} = \mathbf{X}\boldsymbol\beta.
\end{equation*}
We can then re-write the response in terms of the model and noise as
\begin{align*}
\label{eq:linear_regression}
    \mathbf{y} 
	&=\boldsymbol{\tilde y} + \boldsymbol{\epsilon}\\
	&=\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}.
\end{align*}




\subsubsection{Ordinary least squares (OLS)}
%DONE
The method of ordinary least squares computes the unique line that minimises the sum of squared differences between the true data and that line.
In other words we have an optimisation problem on the form
\begin{equation*}
	{\displaystyle \min_{\boldsymbol{\beta}
	\in{\mathbb{R}}^{p}}}\frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2
	=\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\vert\vert_2^2,
\end{equation*}
where we have used the definition of a norm-2 vector, that is
\begin{equation*}
	\vert\vert \boldsymbol{x}\vert\vert_2 = \sqrt{\sum_i x_i^2}.
\end{equation*}
We use this to define the cost function of OLS as
\begin{equation}
\label{eq:cost_ols}
	C(\boldsymbol{X}, \boldsymbol\beta) 
	=\frac{1}{n}\vert\vert \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\vert\vert_2^2,
\end{equation}
The $\boldsymbol\beta$ that optimizes this we call $\hat{\boldsymbol\beta}_{OLS}$, and looking at the cost function we see that this will be when the gradient is zero. 

\subsubsection{Ridge}
%DONE
The ordinary least squares method can be inaccurate when the model have highly correlated independent variables. 
Ridge regression tries to solve this by adding a regularization parameter $\lambda$, called a hyperparameter, to the optimization problem. 
We start by re-writing the optimization problem as 
\begin{equation*}
	{\displaystyle \min_{\boldsymbol{\beta}\in
	{\mathbb{R}}^{p}}}\frac{1}{n}\vert\vert \boldsymbol{y}
	-\boldsymbol{X}\boldsymbol{\beta}\vert\vert_2^2+\lambda\vert\vert \boldsymbol{\beta}\vert\vert_2^2,
\end{equation*}
where we require that $\vert\vert \boldsymbol{\beta}\vert\vert_2^2\le t$, where $t$ is a finite number larger than zero. Our cost function the becomes
\begin{equation}
\label{eq:cost_ridge}
	C(\boldsymbol{X},\boldsymbol{\beta}
	)=\frac{1}{n}\vert\vert \boldsymbol{y}
	-\boldsymbol{X}\boldsymbol{\beta}\vert\vert_2^2+\lambda\vert\vert \boldsymbol{\beta}\vert\vert_2^2,
\end{equation}

As with the OLS this is minimized when the gradient is zero, and we call the $\boldsymbol\beta$ that optimizes this for $\hat{\boldsymbol\beta}_{Ridge}$. 

\subsubsection{Closed-form solutions}
%NOT DONE
\begin{comment}
Both the OLS- and Ridge-methods have closed-form analytical solutions. 
These can easily be found by deriving the cost function with respect to $\boldsymbol{\beta}$. 
\end{comment}

\subsection{Gradient methods}
%NOT STARTED
%Something general about gradient methods

\subsubsection{Plain gradient decent}
%NOT STARTED
%This function is for plain/steepest gradient decent. Can be replaced by
%general algorithm instead if wanted.
\begin{lstlisting}
FUNCTION plain_gradient_decent(X, y, hyperparameters, eta, Niterations, epsilon)
	beta = random_number()

	step_size = inf
	iter = 0
	WHILE iter < Niterations AND step_size >= epsilon
		gradients = calculate_gradients(X, y, beta, hyperparameters)
		step_size = eta * gradients
		beta = beta - step_size
		iter = iter + 1
	END WHILE
	return beta
END FUNCTION
\end{lstlisting}





\subsubsection{Stochastic gradient decent}
%NOT DONE
Strictly speaking stochastic gradient decent is taking one sample per step. 
However it is much more common to select a small subset of data, called a
mini-batch, at each step.






\subsubsection{SGD with momentum}
One simple way of improving the convergence rate of the SGD method, is to
include a momentum term: 
\begin{equation*}
    \theta _{t+1} = \theta _t - \eta \nabla_\theta E(\theta )+\gamma (\theta_t
    -\theta_{t-1}     ) ,
\end{equation*}
where $\gamma $ is a momentum parameter, with $0\leq \gamma \leq1$. That last
term serves as a memory term. Our goal is to find the
global minima where the gradient is zero. Our ordinary gradient descent method
is sensitive to noise, and local variations in our data.
This often lead to a behavior where our values for $\theta $ oscillates
towards the global minima. % TODO: create figure  
By introducing a momentum term our gradient method will better be able to move
directly towards the global minima. This is due to information about the previous
gradients in our update scheme.     

