\subsection{Gradient descent polynomial fitting}
We will use gradient descent implemented in Python to fit a second order polynomial 
\begin{equation}
f(x)=x^{2}+1
\label{eq:polynomial_A}
\end{equation}
with \(100\) linearly spaced points \(x\epsilon [-1, 1]\). The data are then 
split in to training and test sets with \(80\%\) and \(20\%\) randomly selected 
data respectively. 

Our prediction is the matrix product of the second order polynomial design matrix 
\(\bf{X}\) and corresponding coefficient column vector \(\bf{\theta }\)
\begin{equation}
	\bf{y _{pred}} = \bf{X}\bf{\theta}.
	\label{eq:y_pred}
\end{equation}
\(\bf{\theta }\) were initialized with values sampled from a normal distibution with 
mean \(0\) and variance \(1\).   
The gradients were calculated with respect to 
\(\bf{\theta }\) by \verb|autograd's| \verb|grad| function on 
the Ordinary Least Squares or Ridge (if L2 regularization parameter \(\lambda \neq 0 \) ) cost function.

For each epoch in stochastic gradient descent, a minibatch of size $m$ were randomly sampled $n/m$ 
times without replacement from the data of size $n$, such that every data point were utilized in 
each epoch. 

To find the best prediction, we analyzed the mean squared error resulting from different 
parameters described in table \ref{tab:GD_parameters_run_1_2} and table \ref{tab:GD_parameters_run_3_4}.

\begin{table}[H]
    \centering
    \caption{Parameters utilized in polynomial fitting using gradient descent for run 1 and 2}  
    \label{tab:GD_parameters_run_1_2} 
\begin{tabular}{c@{\hspace{1cm}} c@{\hspace{1cm}} c}
	\hline 
	Parameter & Run 1 & Run 2\\
	\hline 
	$\eta$  & \verb|linspace(0.6, 1, 5)| & \verb|linspace(0.1, 1, 10)| \\
	$\lambda$ & $(0, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1})$ & 0  \\
	$\gamma$  & 0.1 & (0, 0.2, 0.4, 0.6, 0.8) \\ 
	gd epochs & 100 & N/A \\
	sgd epochs & 25 & 25 \\
	mini batch size & 20 & 20 \\
	RMSprop rho & N/A & 0.99 \\
	ADAM (beta1, beta2) & N/A & N/A \\
	\hline 
\end{tabular}
\end{table}


\begin{table}[H]
    \centering
    \caption{Parameters utilized in polynomial fitting using gradient descent for run 3 and 4. The parameter values in square brackets "[]" for the tuning tuning methods in the column title in corresponding order}  
    \label{tab:GD_parameters_run_3_4} 
\begin{tabular}{c@{\hspace{1cm}} c@{\hspace{1cm}} c}
	\hline 
	Parameter & Run 3 & Run 4: [Plain, AdaGrad, RMSprop, ADAM]\\
	\hline 
	$\eta$  & \verb|logspace(-3, 0, 10)| & [0.9, 0.9, 0.9, 0.2] \\
	$\lambda$ & 0 & 0 \\
	$\gamma$  & (0, 0.2, 0.4, 0.6, 0.8) & [0.4, 0.4, 0.4, 0] \\ 
	gd epochs & N/A & N/A \\
	(mini batch size, sgd epochs) & (20, 25) & ((2, 5), (5,12), (10,25), (80,200)) \\
	RMSprop rho & N/A & 0.99 \\
	ADAM (beta1, beta2) & (0.9, 0.99) & (0.9, 0.99) \\
	\hline 
\end{tabular}
\end{table}

\begin{table}[H]
    \centering
    \caption{Parameters utilized in polynomial fitting using gradient descent for run 3 and 4. The parameter values in square brackets "[]" for the tuning tuning methods in the column title in corresponding order}  
    \label{tab:GD_parameters_run_5} 
\begin{tabular}{c@{\hspace{1cm}} c}
	\hline 
	Parameter & Run 5 \\
	\hline 
	$\eta$  & 0.9 \\
	$\lambda$ & 0 \\
	$\gamma$  & 0.4 \\ 
	sgd epochs & 20 \\
	mini batch size & 10 \\
	\hline 
\end{tabular}
\end{table}

The parameters in table \ref{tab:GD_parameters_run_1_2} and \ref{tab:GD_parameters_run_3_4} are 
learning rate $\eta $, L2 regularization parameter $\lambda $, momentum parameter $\gamma $,
number of gradient descent epochs "gd epochs", number of stochastic gradient descent epochs 
"sgd epochs", stochastic gradient descent minibatch size "mini batch size", RMSprop
parameter "RMSprop rho" and ADAM parameters "ADAM (beta1, beta2)". 


