\subsection{Gradient descent polynomial fitting}
We will use gradient descent implemented in Python to fit a second order polynomial 
\begin{equation}
f(x)=x^{2}+1
\label{eq:polynomial_A}
\end{equation}
with \(100\) linearly spaced points \(x\epsilon [-1, 1]\). The data are then 
split in to training and test sets with \(80\%\) and \(20\%\) randomly selected 
data respectively. 

Our prediction is the matrix product of the second order polynomial design matrix 
\(\bf{X}\) and corresponding coefficient column vector \(\bf{\theta }\)
\begin{equation}
	\bf{y _{pred}} = \bf{X}\bf{\theta}.
	\label{eq:y_pred}
\end{equation}
\(\bf{\theta }\) were initialized with values sampled from a normal distibution with 
mean \(0\) and variance \(1\).   
The gradients were calculated with respect to 
\(\bf{\theta }\) by \verb|autograd's| \verb|grad| function on 
the Ordinary Least Squares or Ridge (if L2 regularization parameter \(\lambda \neq 0 \) ) cost function.

For each epoch in stochastic gradient descent, a minibatch of size $m$ were randomly sampled $n/m$ 
times without replacement from the data of size $n$, such that every data point were utilized in 
each epoch. 

To find the best prediction, we analyzed the mean squared error resulting from different 
parameters described in table \ref{tab:GD_parameters_run_1_2} and table \ref{tab:GD_parameters_run_3_4}.

\begin{table}[H]
    \centering
    \caption{Parameters utilized in polynomial fitting using gradient descent for run 1 and 2}  
    \label{tab:GD_parameters_run_1_2} 
\begin{tabular}{c@{\hspace{1cm}} c@{\hspace{1cm}} c}
	\hline 
	Parameter & Run 1 & Run 2\\
	\hline 
	$\eta$  & \verb|linspace(0.1, 1, 5)| & \verb|linspace(0.1, 1, 10)| \\
	$\lambda$ & (0, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}) & 0  \\
	$\gamma$  & 0.1 & 0.1 \\ 
	gd epochs & 100 & N/A \\
	sgd epochs & 25 & (10, 57, 105, 152, 200) \\
	mini batch size & 20 & 20 \\
	RMSprop rho & N/A & 0.99 \\
	ADAM (beta1, beta2) & N/A & (0.9, 0.99) \\
	\hline 
\end{tabular}
\end{table}


\begin{table}[H]
    \centering
    \caption{Parameters utilized in polynomial fitting using gradient descent for run 3 and 4}  
    \label{tab:GD_parameters_run_3_4} 
\begin{tabular}{c@{\hspace{1cm}} c@{\hspace{1cm}} c}
	\hline 
	Parameter & Run 3 & Run 4\\
	\hline 
	$\eta$  & \verb|logspace(-3, -1, 10)| & \verb|linspace(0.1, 1, 10)| \\
	$\lambda$ & 0 & 0 \\
	$\gamma$  & 0.1 & 0.5 \\ 
	gd epochs & N/A & 20 \\
	sgd epochs & (10, 57, 105, 152, 200) & 5 \\
	mini batch size & 20 & 20 \\
	RMSprop rho & 0.99 & 0.99 \\
	ADAM (beta1, beta2) & (0.9, 0.99) & (0.9, 0.99) \\
	\hline 
\end{tabular}
\end{table}
The parameters in table \ref{tab:GD_parameters_run_1_2} and \ref{tab:GD_parameters_run_3_4} are 
learning rate $\eta $, L2 regularization parameter $\lambda $, momentum parameter $\gamma $,
number of gradient descent epochs "gd epochs", number of stochastic gradient descent epochs 
"sgd epochs", stochastic gradient descent minibatch size "mini batch size", RMSprop
parameter "RMSprop rho" and ADAM parameters "ADAM (beta1, beta2)". 


\subsection{Neural Network polynomial fitting}

Our Neural Network implemented in Python was used to fit the same polynomial as the one fitted with 
gradient descent in equation \ref{eq:polynomial_A}, but now with $1000$ linearly 
spaced input values $x \epsilon [-1,1]$. As this is a regression problem, we 
did not use any output activation functions, and the mean squared error was employed 
as the cost function in our neural network. We used sigmoid, relu or leaky relu as activation function 
for the hidden layers. 

To optimize the polynomial fit, we studied the mean squared error of our trained network
with different parameters described in table \ref{tab:NN_polynomial_parameters}.

\begin{table}[htpb]
\centering
\caption{Neural network polynomial fit parameters for run 1 and 2}
\label{tab:NN_polynomial_parameters}
\begin{tabular}{c@{\hspace{1cm}} c@{\hspace{1cm}} c}
	\hline 
	Parameter & Run 1 & Run 2 \\
	\hline 
	eta  & \verb|linspace(0.05, 0.5, 5)| & \verb|logspace(-7, -1, 7)| \\
	depth  & 1 & 1 \\
	width  & 5 & 5 \\
	activation hidden & sigmoid & relu \\
	gamma & 0 & 0 \\
	lambd & (0, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1) &  (0, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1) \\
	n minibatches & 10 & 10 \\
	epochs & 2000 & 2000 \\
	\hline 
\end{tabular}

\end{table}

\begin{table}[htpb]
\centering
\caption{Neural network polynomial fit parameters for run 3 and 4}
\label{tab:NN_polynomial_parameters}
\begin{tabular}{c@{\hspace{1cm}} c@{\hspace{1cm}} c}
	\hline 
	Parameter & Run 3 & Run 4 \\
	\hline 
	eta  & \verb|logspace(-7, -1, 7)| & \verb|linspace(0.1, 2, 5)| \\
	depth  & 1 & 1 \\
	width  & 5 & 5 \\
	activation hidden & leaky relu & sigmoid \\
	gamma & 0 & 0 \\
	lambd & (0, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1) &  (0, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1) \\
	n minibatches & 10 & 10 \\
	epochs & 2000 & 2000 \\
	\hline 
\end{tabular}

\end{table}

\begin{table}[htpb]
\centering
\caption{Neural network polynomial fit parameters for run 5}
\label{tab:NN_polynomial_parameters}
\begin{tabular}{c@{\hspace{1cm}} c}
	\hline 
	Parameter & Run 5  \\
	\hline 
	eta  & \verb|logspace(-6, 0, 7)| \\
	depth  & 1 \\
	width  & 5 \\
	activation hidden & relu \\
	gamma & 0 \\
	lambd & (0, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1) \\
	n minibatches & 10 \\
	epochs & 2000 \\
	\hline 
\end{tabular}

\end{table}
