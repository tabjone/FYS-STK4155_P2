

\subsection{Neural Network polynomial fitting}

Our Neural Network implemented in Python was used to fit the same polynomial as the one fitted with 
gradient descent in equation \ref{eq:polynomial_A}, but now with $1000$ linearly 
spaced input values $x \epsilon [-1,1]$. As this is a regression problem, we 
did not use any activation functions on the output layer, and the mean squared error was employed 
as the cost function in our neural network. We used sigmoid, relu or leaky relu as activation function 
for the hidden layers. The wheights were initialized using a normal distribution with variance 1, and the 
biases were initialized with value $0,01$.  

To optimize the polynomial fit, we studied the mean squared error of our trained network
with different parameters described in table \ref{tab:NN_polynomial_parameters1}-\ref{tab:NN_polynomial_parameters3}.
The reproduce the results, run the file \verb|results_NN_polyfit.py| in the following GitHub repository: \url{https://github.com/fredrikjp/FYS-STK4155/tree/master/Project2}.

\begin{table}[htpb]
\centering
\caption{Neural network polynomial fit parameters for run 1 and 2}
\label{tab:NN_polynomial_parameters1}
\begin{tabular}{c@{\hspace{1cm}} c@{\hspace{1cm}} c}
	\hline 
	Parameter & Run 1 & Run 2 \\
	\hline 
	eta  & \verb|linspace(0.05, 0.5, 5)| & \verb|logspace(-7, -1, 7)| \\
	depth  & 1 & 1 \\
	width  & 5 & 5 \\
	activation hidden & sigmoid & relu \\
	gamma & 0 & 0 \\
	lambd & $(0, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1)$ &  $(0, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1)$ \\
	n minibatches & 10 & 10 \\
	epochs & 2000 & 2000 \\
	\hline 
\end{tabular}

\end{table}

\begin{table}[htpb]
\centering
\caption{Neural network polynomial fit parameters for run 3 and 4}
\label{tab:NN_polynomial_parameters2}
\begin{tabular}{c@{\hspace{1cm}} c@{\hspace{1cm}} c}
	\hline 
	Parameter & Run 3 & Run 4 \\
	\hline 
	eta  & \verb|logspace(-7, -1, 7)| & \verb|linspace(0.1, 2, 5)| \\
	depth  & 1 & 1 \\
	width  & 5 & 5 \\
	activation hidden & leaky relu & sigmoid \\
	gamma & 0 & 0 \\
	lambd & $(0, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1)$ &  $(0, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1)$ \\
	n minibatches & 10 & 10 \\
	epochs & 2000 & 2000 \\
	\hline 
\end{tabular}

\end{table}

\begin{table}[htpb]
\centering
\caption{Neural network polynomial fit parameters for run 5}
\label{tab:NN_polynomial_parameters3}
\begin{tabular}{c@{\hspace{1cm}} c@{\hspace{1cm}} c}
	\hline 
	Parameter & Run 5 & Run 6  \\
	\hline 
	eta  & \verb|logspace(-6, 0, 7)| & 0.1 \\
	depth  & 1 & 1 \\
	width  & 5 & 5 \\
	activation hidden & relu & sigmoid \\
	gamma & 0 & 0 \\
	lambd & $(0, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1)$ & 0 \\
	n minibatches & 10 & 10 \\
	epochs & 2000 & 2000 \\
	\hline 
\end{tabular}

\end{table}

For benchmarking we used \verb|sklearn MLPRegressor| on "etaa" array with different learning rates
and "lambd" array with different L2 regularization parameters as shown in the code below:

\begin{lstlisting}[language=Python]
from sklearn.linear_model import LogisticRegression

j=0
for eta in etaa:
    i = 0
    for lmb in lambd:
        NN_sigmoid = MLPRegressor(hidden_layer_sizes=(5), activation="logistic", solver="sgd", alpha=lmb, batch_size = batch_size, learning_rate_init = eta, momentum = 0, max_iter=n_epochs ,n_iter_no_change=2000)
        NN_sigmoid.fit(x_train, y_train.ravel())  # x_train column vector with input values, y_train column vector with target values
        MSE[j, i] = NN_sigmoid.loss_  
        i+=1
    j+=1
\end{lstlisting}

