
% Multi-layer perception aka. dense neural network


\subsection{Neural network}
A neural network is a computer system that works kind of like the brain. It is
composed of a set of interconnected processing nodes, or neurons, that
activates using weight and biases. This network can be dense, meaning that all
neurons in a layer is connected to all neurons in the previous and next layer.
The reason they are called neuron is that they mimics the behavior of a
biological neuron. We will look specifically at a feed-forward multi-layer
perception (MPL) neural network, often called a dense neural network. 


\subsubsection{Initialization}
% Weights and biases

One of the most important aspects of training a neural network is the
initialization of the weights and biases. This process sets the starting point
for the network and can have a significant impact on the performance of the
model. There are a number of different methods that can be used to initialize
the weights and biases, and the choice of method can be critical for the
success of the training process. Some of the more common methods include random
initialization, Xavier initialization, and He initialization.
\\
We will use random initialization, meaning we will set the weights using a
random distribution. And the biases we will set to $0.001$. It is common to
initialize the biases as a zero, or a small number to assure that the network
don't die. We also need to be careful to not set the weights to high as this
might cause the network to explode. 


\subsubsection{Feed-forward}
Feed-forward means that the processing of information in the network only flows
trough the nodes in one direction, from the input layer to the output layer.
\\
To implement this we let the first layer of the network get the input. Then we
use an activation function on that layer, send that to the next layer,
activation, ..., and so on until we reach the output layer. Mathematically
\begin{equation*}
    a_L = \sigma((z_L))=\sigma{(a_{L-1})=\sigma{(\sigma(z_{L-1}))}}=....
\end{equation*}
until we reach the first layer, and the input. Next we will talk about these activation
functions.

\subsubsection{Activation functions}
%
A property that characterizes a neural network, other than its connectivity, is
the choice of activation function(s). 
The following restrictions are imposed on an activation function for a FFNN to
fulfill the universal approximation theorem.
- Non-constant
- Bounded
- Monotonically-increasing
- Continuous

The second requirement excludes all linear functions. Furthermore, in a MLP
with only linear activation functions, each layer simply performs a linear
transformation of its inputs.

Regardless of the number of layers, the output of the NN will be nothing but a
linear function of the inputs. Thus we need to introduce some kind of
non-linearity to the NN to be able to fit non-linear functions. This is
typically done with the Sigmoid or hyperbolic tangent function.
\\~\\
\textbf{Sigmoid and Hyperbolic tangent}:
The sigmoid activation function
\begin{equation*}
    \label{eq:sigmoid} 
    \sigma(z) = \frac{1}{1+e^{-z}}
\end{equation*}
is an ideal activation function for a hidden layer given it's easy derivative

\begin{equation*}
    \sigma'(z) = (1-\sigma(z))z.
\end{equation*}
This is a widely used activation function, but one must be careful of overflow
due to the exponential term. The hyperbolic tangent activation function
\begin{equation*}
    \sigma(z) = tanh(z),
\end{equation*}
with the derivative
\begin{equation*}
    \sigma'(z) = sech^2(z)
\end{equation*}
behaves much like the sigmoid except is has an output from negative one to plus
one, compared to the sigmoid which has an output from zero to one. Both of
these functions have a problem in many-layer networks, and that is the
vanishing gradients problem. Therefore in many-layer networks other functions
are ideal, such as ReLU.
\\~\\
\textbf{ReLU}:
The rectified linear unit (ReLU) is piecewise linear and will output the input
directly if it is positive and will output zero if not. It is as follows
\begin{equation*}
    \sigma(z) = argmax(0, z),
\end{equation*}
and has a split derivative, where $\sigma'(z)=1$ if $z\geq 0$ or else it is zero.
This is also an ideal activation function for a hidden layer and does much
better than sigmoid or hyperbolic tangent at many-layer networks. It is the
default activation function for MLPs and convolutional neural networks (CNN).
And by solving the vanishing gradients problem it makes the network learn
faster and preform better.
\\~\\
%
\textbf{Leaky ReLU}:
The leaky ReLU activation function has a small slope for negative values
instead of zero. So that $\sigma(x)=ax$ when $x<0$ and this slope is determined
before training. In other words it is not a hyperparameter. Other than that it
is exactly as ReLU.
\\~\\
%
\textbf{Softmax}:
The Softmax activation function is a little different than the other functions
in that it produces a normalized probability distribution over the predicted
output classes and it is therefore often used as the activation for the output
layer of the network.
\begin{equation*}
    \sigma(\vec{z})_i=\frac{e^{z_i}}{\Sigma_{j=1}^{K}e^{z_j}}.
\end{equation*}

This inputs a vector $\vec{z}$ and outputs a normalized probability
distribution. The probability distribution is handled in the numerator and the
normalization in the denominator. It is also often useful to re-define
\begin{equation*}
    z_i \gets z_i - argmax(\vec{z_i})
\end{equation*}
as to prevent overflow in the exponential. This will not affect the
probabilities. 

\subsubsection{Training and evaluation}
To train the neural network we do what is called back-propagation. This is a
way to adjust the weights and biases to minimize the error in the predictions
made by the network. The steps of the back-propagation algorithm is as follows.
First we calculate the error in the $L$'th layer, the output layer
\begin{equation*}
    \delta_j^L = f'(z_j^L)\frac{\partial {\cal C}}{\partial (a_j^L)}.
\end{equation*}
Then we compute the back propagate error for each $l=L-1,L-2,\dots,2$ as
\begin{equation*}
    \delta_j^l = \sum_k \delta_k^{l+1}w_{kj}^{l+1}f'(z_j^l).
\end{equation*}
Finally, we update the weights and the biases using gradient descent for each
$l=L-1,L-2,\dots,2$ and update the weights and biases according to the rules

$$
w_{jk}^l\leftarrow  = w_{jk}^l- \eta \delta_j^la_k^{l-1},
$$

$$
b_j^l \leftarrow b_j^l-\eta \frac{\partial {\cal C}}{\partial b_j^l}=b_j^l-\eta
\delta_j^l.
$$
We keep doing this until we reach some max number of training steps or the
gradients gets sufficiently small.

To evaluate the performance of the network we will look at the square of the
difference between the wanted outcome, or target, $t$ and the output of the
network. We call this the accuracy of the network.

\subsection{Classification problems} \label{sec:method_classification} 
Classes in neural networks are defined by the output of the network. For
example, if the output of the network is a single class, then the network is
said to be a single-class network. If the output of the network is two classes,
then the network is said to be a two-class network. And so on.

A common classification problem is image classification. In
this problem, the input to the network is an image, and the output is the class
of the image. For example, the output might be “cat”, “dog”, “bird”, etc.

We have chosen to train our network using the Wisconsin 
\href{https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load\_breast\_cancer.html?fbclid=IwAR0RNzOImikVXi41ecb14u\_qvUDybyIII43e9ySk0GEjyYWyPzybmmHeQWs}{\underline{breast
cancer dataset}} by learn.

There are 569 samples in the Wisconsin breast cancer data set and it has two
classes, malignant and benign. Each of the samples has 30 features such as the
patients’ age, the stage of their cancer, size of the tumor, etc. This dataset
is very useful in neural network classification problems because it is a well-known
data set that has been used extensively in research. Additionally, the data set
is small enough that it can be used to train a neural network without requiring
much computational power.


% Methods: 
% Normalizing data 
% splitting of data 


% XXX: Scaling after split

All code was implemented and analyzed in Python. Thus function names refers to
specify python packages and modules.  

Our cancer data was split into training and test data, with a test size of
20\%. 
Scikit-learn's \verb|train_test_split| method was used for this purpose. 


Before attempting to train the network the data was scaled, since each
feature lives on different Scales. All 30 features was scaled by subtracting
the mean and dividing by the standard deviation, with scikit-learns
\verb|StandardScaler| object. 

In order to keep the results consistent between runs, a seed value of 5 was set
with the nappy's random module. 


% Output layers Cost score
Since our data only consist if two classes, malignant and benign we will use
the binary Cross Entropy as a cost function. Where our targets is a column vector
with 0 and 1 (benign and malignant) % FIXME: or opposite



% Activation output layer
The Sigmoid function (see \eqref{eq:sigmoid}) was used as our activation
function in the output Layer.
We can interpret the sigmoid function as probability function. 
We then define the output as the probability that our prediction belongs to
class 0: 
\begin{equation*}
    P(y = 0 | z^L, \theta ) = \sigma (z^L),
\end{equation*}
where $\theta $ is the weights and the bias, and $z^L$  is the input to the
activation function. $\sigma $ is the sigmoid function. 
For our binary classification problem it follows that: 
\begin{equation*}
    P(y = 1 | z^L, \theta ) = 1 - \sigma (z^L)
\end{equation*}


% TODO: accuracy score

% Table with params
In order to find the optimal values for hyper-parameteres, multiple runs with
different combination of hyper-parameter was used. All runs with it's specific
hyper-parameters is listed in table \ref{tab:runs_classification_cancer}. The different alorithms %XXX ref 



% FIXME: need reference the different parameters
\begin{table}[H]
    \centering
    \caption{Different runs used to find the optimal parameters of our NN in
    classification of breast cancer data}  
    \label{tab:runs_classification_cancer} 
    \begin{tabular}{|c|c|c|c|c|}
        \hline

        Parameter & Run 1 & Run 2 & Run 3 \\
        \hline
        eta & 0.001 & 0.00001, 0.0001, 0.001, 0.01, 0.1 & 0.1 \\
        \hline
        depth & 1  & 1  & 1, 2, 3 \\
        \hline
        width & 10   & 10 & 5, 10, 15, 20\\
        \hline
        activation hidden & sigmoid & sigmoid & sigmoid\\
        \hline
        gamma & 0.9 & 0.9 & 0.9\\
        \hline
        lambd & 0.0  & 0, 0.0001, 0.001, 0.01, 0.1 & 0.1\\
        \hline
        tuning method & none & none  & none\\
        \hline
        n mini batches & 1, 5, 10, 15, 20  & 20 & 20\\
        \hline
        epochs & 400  & 200 & 200\\
        \hline
         
    \end{tabular} 
\end{table}

\begin{table}[H]
    \centering
    \caption{Different runs used to find the optimal parameters of our NN in
    classification of breast cancer data}  
    \label{tab:runs_classification_cancer2} 
    \begin{tabular}{|c|c|c|c|c|}
        \hline

        Parameter & Run 4 & Run 5 \\
        \hline
        eta & 0.001 & 0.001\\
        \hline
        depth & 2  & 2\\
        \hline
        width & 5   & 5\\
        \hline
        activation hidden & sigmoid & sigmoid, relu, leaky relu\\
        \hline
        gamma & 0.9, 0 & 0\\
        \hline
        lambd & 0  & 0\\
        \hline
        tuning method & none, adam, rms prop, adagrad  & none\\
        \hline
        n mini batches & 20  & 20\\
        \hline
        epochs & 200  & 200\\
        \hline
         
    \end{tabular} 
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Logistic regression}

Logistic regression is a statistical method for classifying data into two
groups. It is often used in binary classification problems.

Logistic regression works by using a logistic function to map the data points
onto a curve. The logistic function is a sigmoid function (equation
\ref{eq:sigmoid}), which takes any real
valued number and maps it onto a value between 0 and 1. 
Values above 0.5 is assigned to class 1, and below 0.5 to class 0. 

The logistic function can be used for any number of predictors. For multiple
predictors the function takes the form: 
\begin{equation*}
    \label{eq:log_reg} 
    \sigma (z) = \frac{1}{1+e^{-z}} =\frac{1}{1+e^{-(\beta_0 + \beta _1 x_1+ \beta_2 +x_2 + \ldots +
    \beta _p x_p)}}, 
\end{equation*}
where p is the number of predictors. Our problem is then to fit the
coefficients $\beta _i$ for each feature $x_i$ such that our cost function is
minimized.  

Our developed FINN code can easy be reconfigured to a logistic regression
solver as discussed below.

We will again use the Wisconsin Breast Cancer data for our logistic regression
problem. The full dataset was used with all 30 features. That data was split
and standardized with the same random seed value as in section
\ref{sec:method_classification}. 

All hidden layers was the removed from the neural network. Thus, our neural
network consisted of an input layer outputting 30 feature vectors, and an
output layer of one node and the sigmoid function as the activation function.
Our feed forward pass in the network, therefore takes the form as in equation
\ref{eq:log_reg}, where $\beta _i$ is the weights into the output layer. 
Again, we used cross entropy as our cost function.  






